{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by step (Resumen)\n",
    "\n",
    "- Lanzamos cluster en aws\n",
    "- Nos conectamos por consola y ssh al cluster desde el pc personal\n",
    "- Instalamos jupyterhub y clonamos el  [repo](https://github.com/camilaMejia/trabajoFinal) del proyecto con este notebook listo. (En el github hay un archivo que se llama launch.txt donde están todas las intrucciones que se lanzan por comando)\n",
    "- Instalamos y cargamos todas las librerias necesarias.\n",
    "- Nos traemos el .dat y el .csv desde S3 al almacenamiento local\n",
    "- Creamos el indice invertido usando metapy\n",
    "- Hacemos querying usando BM25\n",
    "- Se hace un LDA con todos las noticias (solo content + title)\n",
    "- Para cada noticia hacemos vemos cual es el topico dominante\n",
    "- Asignación de la polaridad de cada noticia utilizando textBlob.\n",
    "\n",
    "\n",
    "\n",
    "# Procedimiento en AWS\n",
    "\n",
    "Uno de los objetivos de este trabajo estaba asociado a la posibilidad de correr todo esto en maquinas virtuales y clusters de AWS. Para ello lo que hicimos fue la creación de un cluster a traves de EMR y luego hacer un ambiente particular para instalar metapy.\n",
    "\n",
    "Una dificultad es que estos clusters son efimeros por lo que no almacenan los notebooks, por eso lo que se hizo es que cuando lanzamos el cluster además de instalar metapy y el resto de librerias necesarias instalamos github para clonar todos los archivos necesarios menos el .dat y el csv con todas las noticias (dado que son muy pesados).\n",
    "\n",
    "Para traer al ambiente los datos que son muy grandes lo que hacemos es copiarlos de una carpeta de aws.\n",
    "\n",
    "Para la parte de LDA con todos los datos era necesario correr spark en el cluster (no local) por eso para esta tarea se usaron los notebooks nativos de EMR. En la versión que vemos en este notebook en realidad solo tenemos un demo co 1000 noticias para que corra suficientmeente rapido (además que el EMR consume muchos más recursos y los 50USD no son suficiente).\n",
    "\n",
    "# Indexación y recuperación: metapy\n",
    "\n",
    "Usando la libreria metapy lo que hacemos es simplemente configurar el archivo llamado minifig.toml en el cual especificamos en donde están los documentos que vamos a indexar (junto con su formato) para luego hacer la recuperación.\n",
    "\n",
    "Lo que se hace en la sección de indexación es la creación del indice invertido que luego va a ser utilizado, en la sección de recuperación simplemente se crea un objeto  el cual genera un ranking (en este caso es un BM25) cuando se le pasa un query.\n",
    "\n",
    "# LDA: spark\n",
    "\n",
    "Lo que hacemos en este procedimiento es usar las funciones de spark para leer la información de todas las noticias para primero hacer la representación vectorial de los documentos, luego calculamos el indice invertido para finalmente hacer la reducción de dimencionalidad con LDA.\n",
    "\n",
    "LDA lo que hace es que asigna a cada noticia un vector de k posiciones en donde la posicion i  expresa la probabilidad de que la noticia hable del topico i. Además de esto a cada noticia se le puede asignar su topico dominante.\n",
    "\n",
    "Adicional vemos que cada topico tiene unas tokens que lo definen, dado que estos topicos no son predeterminados es necesario que se haga un analisis de qué es cada cluster con base en esos tokens representativos (algo parecido a cuando en k-means se hace un analisis de los centroides).\n",
    "\n",
    "Algo muy importante es que en este demo es necesario crear un contexto de spark (que además es local), por el contrario en EMR el contexto ya es predeterminado.\n",
    "\n",
    "# Sentiment Analysis\n",
    "\n",
    "Para esta sección la idea inicial era entrenar un algoritmo sencillo, pero dado que no hay un sentimiento asociado (label), optamos por la opción de utilizar una libreria que nos da una nivel de sentimiento en cada noticia según su polaridad (negativo o positivo).\n",
    "\n",
    "La idea es usar textBlob y con base en la distribución de la polaridad vamos a asignar un sentimiento a cada noticia: Negativo, neutral o positivo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalar librerias y complementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/hadoop/env/lib64/python3.6/site-packages (0.24.2)\n",
      "Requirement already satisfied: pytz>=2011k in /home/hadoop/env/lib/python3.6/site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/hadoop/env/lib64/python3.6/site-packages (from pandas) (1.16.4)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /home/hadoop/env/lib/python3.6/dist-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/hadoop/env/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "Requirement already satisfied: pyspark in /home/hadoop/env/lib/python3.6/site-packages (2.4.3)\n",
      "Requirement already satisfied: py4j==0.10.7 in /home/hadoop/env/lib/python3.6/site-packages (from pyspark) (0.10.7)\n",
      "Requirement already satisfied: metapy in /home/hadoop/env/lib64/python3.6/site-packages (0.2.13)\n",
      "Requirement already satisfied: boto3 in /home/hadoop/env/lib/python3.6/site-packages (1.9.172)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.172 in /home/hadoop/env/lib/python3.6/site-packages (from boto3) (1.12.172)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/hadoop/env/lib/python3.6/site-packages (from boto3) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /home/hadoop/env/lib/python3.6/site-packages (from boto3) (0.2.1)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version >= \"3.4\" in /home/hadoop/env/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.172->boto3) (1.25.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/hadoop/env/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.172->boto3) (2.8.0)\n",
      "Requirement already satisfied: docutils>=0.10 in /home/hadoop/env/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.172->boto3) (0.14)\n",
      "Requirement already satisfied: six>=1.5 in /home/hadoop/env/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.172->boto3) (1.12.0)\n",
      "Requirement already satisfied: nltk in /home/hadoop/env/lib/python3.6/site-packages (3.4.3)\n",
      "Requirement already satisfied: six in /home/hadoop/env/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: numpy in /home/hadoop/env/lib64/python3.6/site-packages (1.16.4)\n",
      "Requirement already satisfied: matplotlib in /home/hadoop/env/lib64/python3.6/site-packages (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/hadoop/env/lib/python3.6/dist-packages (from matplotlib) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /home/hadoop/env/lib64/python3.6/site-packages (from matplotlib) (1.16.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/hadoop/env/lib64/python3.6/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/hadoop/env/lib/python3.6/site-packages (from matplotlib) (2.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/hadoop/env/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/hadoop/env/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /home/hadoop/env/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (41.0.1)\n",
      "Requirement already satisfied: TextBlob in /home/hadoop/env/lib64/python3.6/site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in /home/hadoop/env/lib/python3.6/site-packages (from TextBlob) (3.4.3)\n",
      "Requirement already satisfied: six in /home/hadoop/env/lib/python3.6/dist-packages (from nltk>=3.1->TextBlob) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas\n",
    "! pip install pyspark\n",
    "! pip install metapy\n",
    "! pip install boto3\n",
    "! pip install nltk\n",
    "! pip install numpy\n",
    "! pip install matplotlib\n",
    "! pip install TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "import metapy\n",
    "import requests, zipfile, io, os, boto3\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    " \n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA, BisectingKMeans\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import re\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "sc = SparkContext('local', \"app-topic-detection\") \n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar datos necesarios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 =  boto3.client('s3', region_name='us-east-1')\n",
    "with open('data/news/news.dat', 'wb') as f:\n",
    "    s3.download_fileobj('finaltext','news.dat', f)\n",
    "\n",
    "\n",
    "\n",
    "obj = s3.get_object(Bucket='finaltext', Key=u'news.csv')\n",
    "df = pd.read_csv(obj['Body'])\n",
    "\n",
    "df['all']=df.title + df.content\n",
    "\n",
    "df2=df[['all']]\n",
    "df2.to_csv('aux.csv')\n",
    "df2.head(1000).to_csv('mini.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted index using metapy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf news-idx\n",
    "idx = metapy.index.make_inverted_index('miniconfig.toml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR: Querys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23373</th>\n",
       "      <td>Ann Coulter: I Might Have Been Killed at the B...</td>\n",
       "      <td>Ann Coulter, in a major interview with Vanity ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55967</th>\n",
       "      <td>A Trump Supporter Dwells in Beijing</td>\n",
       "      <td>BEIJING —  Ardent Chinese supporters of Donald...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123632</th>\n",
       "      <td>For Chinese officials, Trump perhaps better th...</td>\n",
       "      <td>In 2010, then Secretary of State Hillary Clin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119243</th>\n",
       "      <td>Massachusetts college apologizes for racist tw...</td>\n",
       "      <td>Salem State University President Patricia Mese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25871</th>\n",
       "      <td>Donald Trump’s Hypocrisies - Breitbart</td>\n",
       "      <td>Part of Donald Trump’s appeal as a candidate i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "23373   Ann Coulter: I Might Have Been Killed at the B...   \n",
       "55967                 A Trump Supporter Dwells in Beijing   \n",
       "123632  For Chinese officials, Trump perhaps better th...   \n",
       "119243  Massachusetts college apologizes for racist tw...   \n",
       "25871              Donald Trump’s Hypocrisies - Breitbart   \n",
       "\n",
       "                                                  content  \n",
       "23373   Ann Coulter, in a major interview with Vanity ...  \n",
       "55967   BEIJING —  Ardent Chinese supporters of Donald...  \n",
       "123632   In 2010, then Secretary of State Hillary Clin...  \n",
       "119243  Salem State University President Patricia Mese...  \n",
       "25871   Part of Donald Trump’s appeal as a candidate i...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker = metapy.index.OkapiBM25()\n",
    "query = metapy.index.Document()\n",
    "query.content('Trump hates china') # query from AP news\n",
    "top_docs = ranker.score(idx, query, num_results=5)\n",
    "\n",
    "index=[tup[0] for tup in top_docs]\n",
    "df.loc[index,['title','content']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA on spark\n",
    "\n",
    "### Pre process data\n",
    "\n",
    "Here we load data to spark and make some preprocessing over the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "|_c0|                 all|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|House Republicans...|[hous, republican...|\n",
      "|  1|Rift Between Offi...|[rift, offic, res...|\n",
      "|  2|Tyrus Wong, ‘Bamb...|[tyru, wong, bamb...|\n",
      "|  3|Among Deaths in 2...|[among, death, 20...|\n",
      "|  4|Kim Jong-un Says ...|[kim, jongun, say...|\n",
      "|  5|Sick With a Cold,...|[sick, cold, quee...|\n",
      "|  6|Taiwan’s Presiden...|[taiwan, presid, ...|\n",
      "|  7|After ‘The Bigges...|[biggest, loser, ...|\n",
      "|  8|First, a Mixtape....|[first, mixtap, r...|\n",
      "|  9|Calling on Angels...|[call, angel, end...|\n",
      "| 10|Weak Federal Powe...|[weak, feder, pow...|\n",
      "| 11|Can Carbon Captur...|[carbon, captur, ...|\n",
      "| 12|Mar-a-Lago, the F...|[maralago, futur,...|\n",
      "| 13|How to form healt...|[form, healthi, h...|\n",
      "| 14|Turning Your Vaca...|[turn, vacat, pho...|\n",
      "| 15|As Second Avenue ...|[second, avenu, s...|\n",
      "| 16|Dylann Roof Himse...|[dylann, roof, re...|\n",
      "| 17|Modi’s Cash Ban B...|[modi, cash, ban,...|\n",
      "| 18|Suicide Bombing i...|[suicid, bomb, ba...|\n",
      "| 19|Fecal Pollution T...|[fecal, pollut, t...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#rawdata=spark.read.csv('aux.csv', inferSchema=True, header=True)\n",
    "\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "rawdata = spark.read.load(\"mini.csv\", format=\"csv\", header=True)\n",
    "\n",
    "rawdata[\"all\"].cast(StringType())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cleanup_text(record):\n",
    "    text  = record[1]\n",
    "    uid   = record[0]\n",
    "    \n",
    "    # Default list of Stopwords\n",
    "    sw = list(stop_words_nltk)\n",
    "    \n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [ re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens ] #Just letters and numbers\n",
    "    tokens = [ w.lower() for w in tokens] #lowercase\n",
    "    tokens = [ w for w in tokens if (len(w)>1) ] #Not single letter words\n",
    "    tokens = [ w for w in tokens if w not in sw ] #Remove stopwords\n",
    "    stemmer = PorterStemmer ()\n",
    "    tokens =[stemmer.stem(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "udf_cleantext = udf(cleanup_text , ArrayType(StringType()))\n",
    "clean_text = rawdata.withColumn(\"words\", udf_cleantext(struct([rawdata[x] for x in rawdata.columns])))\n",
    "clean_text.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedings + LDA\n",
    "\n",
    "here we create the features of each line and then make the LDA itself with k topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequency Vectorization  - Option 2 (CountVectorizer)    : \n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\", vocabSize = 1000)\n",
    "cvmodel = cv.fit(clean_text)\n",
    "featurizedData = cvmodel.transform(clean_text)\n",
    "\n",
    "vocab = cvmodel.vocabulary\n",
    "vocab_broadcast = sc.broadcast(vocab)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "# Generate 25 Data-Driven Topics:\n",
    "lda = LDA(k=5, seed=123, optimizer=\"em\", featuresCol=\"features\")\n",
    "\n",
    "ldamodel = lda.fit(rescaledData)\n",
    "\n",
    "#model.isDistributed()\n",
    "#model.vocabSize()\n",
    "\n",
    "ldatopics = ldamodel.describeTopics()\n",
    "#ldatopics.show(25)\n",
    "\n",
    "def map_termID_to_Word(termIndices):\n",
    "    words = []\n",
    "    for termID in termIndices:\n",
    "        words.append(vocab_broadcast.value[termID])\n",
    "    \n",
    "    return words\n",
    "\n",
    "udf_map_termID_to_Word = udf(map_termID_to_Word , ArrayType(StringType()))\n",
    "ldatopics_mapped = ldatopics.withColumn(\"topic_desc\", udf_map_termID_to_Word(ldatopics.termIndices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------+\n",
      "|topic|topic_desc                                                                    |\n",
      "+-----+------------------------------------------------------------------------------+\n",
      "|0    |[polic, attack, women, ms, march, kill, museum, drug, black, dr]              |\n",
      "|1    |[trump, republican, russia, russian, health, mr, intellig, obama, insur, care]|\n",
      "|2    |[china, unit, trade, refuge, trump, state, european, iran, mexico, countri]   |\n",
      "|3    |[judg, court, senat, feder, compani, justic, school, gorsuch, rule, law]      |\n",
      "|4    |[neanderth, book, game, food, play, app, data, player, season, chines]        |\n",
      "+-----+------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ldatopics_mapped.select(ldatopics_mapped.topic, ldatopics_mapped.topic_desc).show(50,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add detected topic to each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                 all|               words|            features|   topicDistribution|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|House Republicans...|[hous, republican...|(1000,[0,1,2,3,4,...|[0.04068071407723...|\n",
      "|Rift Between Offi...|[rift, offic, res...|(1000,[0,1,3,4,5,...|[0.75246269816359...|\n",
      "|Tyrus Wong, ‘Bamb...|[tyru, wong, bamb...|(1000,[0,1,3,4,5,...|[0.41300272484273...|\n",
      "|Among Deaths in 2...|[among, death, 20...|(1000,[0,1,3,4,5,...|[0.26343310317094...|\n",
      "|Kim Jong-un Says ...|[kim, jongun, say...|(1000,[0,1,2,3,5,...|[0.05080174191673...|\n",
      "|Sick With a Cold,...|[sick, cold, quee...|(1000,[1,3,6,9,13...|[0.32384781114285...|\n",
      "|Taiwan’s Presiden...|[taiwan, presid, ...|(1000,[0,1,2,3,4,...|[0.08547173920827...|\n",
      "|After ‘The Bigges...|[biggest, loser, ...|(1000,[0,1,3,4,5,...|[0.26709801958714...|\n",
      "|First, a Mixtape....|[first, mixtap, r...|(1000,[0,1,3,4,6,...|[0.37183104241155...|\n",
      "|Calling on Angels...|[call, angel, end...|(1000,[0,1,3,4,6,...|[0.39042831844297...|\n",
      "|Weak Federal Powe...|[weak, feder, pow...|(1000,[0,1,2,3,4,...|[0.07898722773055...|\n",
      "|Can Carbon Captur...|[carbon, captur, ...|(1000,[0,1,2,3,4,...|[0.04296751221521...|\n",
      "|Mar-a-Lago, the F...|[maralago, futur,...|(1000,[0,1,2,3,4,...|[0.14958019766743...|\n",
      "|How to form healt...|[form, healthi, h...|(1000,[3,4,6,7,9,...|[0.14672640250975...|\n",
      "|Turning Your Vaca...|[turn, vacat, pho...|(1000,[1,3,4,5,6,...|[0.05592385036349...|\n",
      "|As Second Avenue ...|[second, avenu, s...|(1000,[0,1,3,4,5,...|[0.39201336277986...|\n",
      "|Dylann Roof Himse...|[dylann, roof, re...|(1000,[0,1,3,4,5,...|[0.36863978175320...|\n",
      "|Modi’s Cash Ban B...|[modi, cash, ban,...|(1000,[0,1,2,3,4,...|[0.13278043610077...|\n",
      "|Suicide Bombing i...|[suicid, bomb, ba...|(1000,[0,1,3,6,7,...|[0.34901223918232...|\n",
      "|Fecal Pollution T...|[fecal, pollut, t...|(1000,[0,1,3,7,10...|[0.17958077045511...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ldaResults = ldamodel.transform(rescaledData)\n",
    "\n",
    "ldaResults.select('all','words','features','topicDistribution').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add principal topic to each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                 all|topic_prin|\n",
      "+--------------------+----------+\n",
      "|House Republicans...|         1|\n",
      "|Rift Between Offi...|         0|\n",
      "|Tyrus Wong, ‘Bamb...|         0|\n",
      "|Among Deaths in 2...|         4|\n",
      "|Kim Jong-un Says ...|         2|\n",
      "|Sick With a Cold,...|         0|\n",
      "|Taiwan’s Presiden...|         2|\n",
      "|After ‘The Bigges...|         4|\n",
      "|First, a Mixtape....|         4|\n",
      "|Calling on Angels...|         0|\n",
      "|Weak Federal Powe...|         3|\n",
      "|Can Carbon Captur...|         3|\n",
      "|Mar-a-Lago, the F...|         4|\n",
      "|How to form healt...|         4|\n",
      "|Turning Your Vaca...|         4|\n",
      "|As Second Avenue ...|         0|\n",
      "|Dylann Roof Himse...|         3|\n",
      "|Modi’s Cash Ban B...|         2|\n",
      "|Suicide Bombing i...|         2|\n",
      "|Fecal Pollution T...|         4|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "def foo(topicDistribution):\n",
    "    dom = topicDistribution[0]\n",
    "    index_dom = 0\n",
    "    for index in range(len(topicDistribution)):\n",
    "        if (topicDistribution[index]>dom):\n",
    "            dom=topicDistribution[index]\n",
    "            index_dom=index\n",
    "    \n",
    "    return index_dom\n",
    "\n",
    "udf_seltop = udf(foo , IntegerType())\n",
    "aaa = ldaResults.withColumn(\"topic_prin\", udf_seltop(ldaResults.topicDistribution))\n",
    "\n",
    "\n",
    "aaa.select('all','topic_prin').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.1 s, sys: 0 ns, total: 14.1 s\n",
      "Wall time: 14.1 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def p_pol(x):\n",
    "    if x > p66:\n",
    "        return \"positivo\"\n",
    "    elif x < p33:\n",
    "        return \"negativo\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "def sentiment_func(news):\n",
    "    try:\n",
    "        return TextBlob(news).sentiment\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "df_pandas=aaa.select('all','topic_prin').toPandas()\n",
    "\n",
    "df=df2.head(df_pandas.shape[0]).copy()\n",
    "    \n",
    "%time df['sentimientos'] = df['all'].apply(sentiment_func)\n",
    "\n",
    "df['polaridad'] = df['sentimientos'].apply(lambda x: x[0])\n",
    "df['Subjetividad'] = df['sentimientos'].apply(lambda x: x[1])\n",
    "\n",
    "\n",
    "p33=np.percentile(df['polaridad'],33)\n",
    "p66=np.percentile(df['polaridad'],66)\n",
    "\n",
    "df['feeling']=df['polaridad'].apply(p_pol)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df=df[['all','polaridad','Subjetividad','feeling']].merge(df_pandas)\n",
    "\n",
    "df.to_csv('Sentiments_news.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
