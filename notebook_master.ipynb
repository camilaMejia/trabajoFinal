{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qué hemos hecho?\n",
    "\n",
    "- Lanzamos cluster en aws\n",
    "- Nos conectamos por consola y ssh al cluster desde el pc personal\n",
    "- Instalamos jupyterhub y clonamos el  [repo](https://github.com/camilaMejia/trabajoFinal) del proyecto con este notebook listo. (En el github hay un archivo que se llama launch.txt donde están todas las intrucciones que se lanzan por comando)\n",
    "- Instalamos y cargamos todas las librerias necesarias.\n",
    "- Nos traemos el .dat y el .csv desde S3 al almacenamiento local\n",
    "- Creamos el indice invertido usando metapy\n",
    "- Hacemos querying usando BM25\n",
    "- Se hace un LDA con todos las noticias (solo content + title)\n",
    "- Para cada noticia hacemos vemos cual es el topico dominante\n",
    "\n",
    "# Con respecto a la entrega\n",
    "\n",
    "- Almacenamiento y Cluster de procesamiento en SparkML/Meta/NLTK en Amazon AWS : Check (Peso 40%)\n",
    "- Indexación, búsqueda y recuperación con META : Check (Peso 40%)\n",
    "- Modelado de tópicos: Check (Peso 10%)\n",
    "- Análisis de sentimientos: Pendiente (Peso 10%)\n",
    "\n",
    "En general estamos al 90% de ejecución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalar librerias y complementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install pandas\n",
    "! pip install pyspark\n",
    "! pip install metapy\n",
    "! pip install boto3\n",
    "!pip install nltk\n",
    "!pip install numpy\n",
    "!pip install re\n",
    "!pip install codecs\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "import metapy\n",
    "import requests, zipfile, io, os, boto3\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    " \n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA, BisectingKMeans\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import re\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "sc = SparkContext('local', \"app-topic-detection\") \n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar datos necesarios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 =  boto3.client('s3', region_name='us-east-1')\n",
    "with open('data/news/news.dat', 'wb') as f:\n",
    "    s3.download_fileobj('finaltext','news.dat', f)\n",
    "\n",
    "\n",
    "\n",
    "obj = s3.get_object(Bucket='finaltext', Key=u'news.csv')\n",
    "df = pd.read_csv(obj['Body'])\n",
    "\n",
    "df['all']=df.title + df.content\n",
    "\n",
    "df2=df[['all']]\n",
    "df2.to_csv('aux.csv')\n",
    "df2.head(1000).to_csv('mini.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted index using metapy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf news-idx\n",
    "idx = metapy.index.make_inverted_index('miniconfig.toml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR: Querys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker = metapy.index.OkapiBM25()\n",
    "query = metapy.index.Document()\n",
    "query.content('Trump hates china') # query from AP news\n",
    "top_docs = ranker.score(idx, query, num_results=5)\n",
    "\n",
    "index=[tup[0] for tup in top_docs]\n",
    "df.loc[index,['title','content']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA on spark\n",
    "\n",
    "### Pre process data\n",
    "\n",
    "Here we load data to spark and make some preprocessing over the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rawdata=spark.read.csv('aux.csv', inferSchema=True, header=True)\n",
    "\n",
    "\n",
    "rawdata = spark.read.load(\"mini.csv\", format=\"csv\", header=True)\n",
    "\n",
    "rawdata[\"all\"].cast(StringType())\n",
    "\n",
    "\n",
    "\n",
    "def cleanup_text(record):\n",
    "    text  = record[1]\n",
    "    uid   = record[0]\n",
    "    try:\n",
    "        words = text.split()\n",
    "    except:\n",
    "        words = 'a about'\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Default list of Stopwords\n",
    "    stopwords_core = ['a', u'about', u'above', u'after', u'again', u'against', u'all', u'am', u'an', u'and', u'any', u'are', u'arent', u'as', u'at', \n",
    "    u'be', u'because', u'been', u'before', u'being', u'below', u'between', u'both', u'but', u'by', \n",
    "    u'can', 'cant', 'come', u'could', 'couldnt', \n",
    "    u'd', u'did', u'didn', u'do', u'does', u'doesnt', u'doing', u'dont', u'down', u'during', \n",
    "    u'each', \n",
    "    u'few', 'finally', u'for', u'from', u'further', \n",
    "    u'had', u'hadnt', u'has', u'hasnt', u'have', u'havent', u'having', u'he', u'her', u'here', u'hers', u'herself', u'him', u'himself', u'his', u'how', \n",
    "    u'i', u'if', u'in', u'into', u'is', u'isnt', u'it', u'its', u'itself', \n",
    "    u'just', \n",
    "    u'll', \n",
    "    u'm', u'me', u'might', u'more', u'most', u'must', u'my', u'myself', \n",
    "    u'no', u'nor', u'not', u'now', \n",
    "    u'o', u'of', u'off', u'on', u'once', u'only', u'or', u'other', u'our', u'ours', u'ourselves', u'out', u'over', u'own', \n",
    "    u'r', u're', \n",
    "    u's', 'said', u'same', u'she', u'should', u'shouldnt', u'so', u'some', u'such', \n",
    "    u't', u'than', u'that', 'thats', u'the', u'their', u'theirs', u'them', u'themselves', u'then', u'there', u'these', u'they', u'this', u'those', u'through', u'to', u'too', \n",
    "    u'under', u'until', u'up', \n",
    "    u'very', \n",
    "    u'was', u'wasnt', u'we', u'were', u'werent', u'what', u'when', u'where', u'which', u'while', u'who', u'whom', u'why', u'will', u'with', u'wont', u'would', \n",
    "    u'y', u'you', u'your', u'yours', u'yourself', u'yourselves']\n",
    "    \n",
    "    # Custom List of Stopwords - Add your own here\n",
    "    stopwords_custom = ['']\n",
    "    stopwords = stopwords_core + stopwords_custom\n",
    "    stopwords = [word.lower() for word in stopwords]    \n",
    "    \n",
    "    text_out = [re.sub('[^a-zA-Z0-9]','',word) for word in words]                                       # Remove special characters\n",
    "    text_out = [word.lower() for word in text_out if len(word)>2 and word.lower() not in stopwords]     # Remove stopwords and words under X length\n",
    "    return text_out\n",
    "\n",
    "udf_cleantext = udf(cleanup_text , ArrayType(StringType()))\n",
    "clean_text = rawdata.withColumn(\"words\", udf_cleantext(struct([rawdata[x] for x in rawdata.columns])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedings + LDA\n",
    "\n",
    "here we create the features of each line and then make the LDA itself with k topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequency Vectorization  - Option 2 (CountVectorizer)    : \n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\", vocabSize = 1000)\n",
    "cvmodel = cv.fit(clean_text)\n",
    "featurizedData = cvmodel.transform(clean_text)\n",
    "\n",
    "vocab = cvmodel.vocabulary\n",
    "vocab_broadcast = sc.broadcast(vocab)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "# Generate 25 Data-Driven Topics:\n",
    "lda = LDA(k=5, seed=123, optimizer=\"em\", featuresCol=\"features\")\n",
    "\n",
    "ldamodel = lda.fit(rescaledData)\n",
    "\n",
    "#model.isDistributed()\n",
    "#model.vocabSize()\n",
    "\n",
    "ldatopics = ldamodel.describeTopics()\n",
    "#ldatopics.show(25)\n",
    "\n",
    "def map_termID_to_Word(termIndices):\n",
    "    words = []\n",
    "    for termID in termIndices:\n",
    "        words.append(vocab_broadcast.value[termID])\n",
    "    \n",
    "    return words\n",
    "\n",
    "udf_map_termID_to_Word = udf(map_termID_to_Word , ArrayType(StringType()))\n",
    "ldatopics_mapped = ldatopics.withColumn(\"topic_desc\", udf_map_termID_to_Word(ldatopics.termIndices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldatopics_mapped.select(ldatopics_mapped.topic, ldatopics_mapped.topic_desc).show(50,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add detected topic to each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaResults = ldamodel.transform(rescaledData)\n",
    "\n",
    "ldaResults.select('all','words','features','topicDistribution').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add principal topic to each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "def foo(topicDistribution):\n",
    "    dom = topicDistribution[0]\n",
    "    index_dom = 0\n",
    "    for index in range(len(topicDistribution)):\n",
    "        if (topicDistribution[index]>dom):\n",
    "            dom=topicDistribution[index]\n",
    "            index_dom=index\n",
    "    \n",
    "    return index_dom\n",
    "\n",
    "udf_seltop = udf(foo , IntegerType())\n",
    "aaa = ldaResults.withColumn(\"topic_prin\", udf_seltop(ldaResults.topicDistribution))\n",
    "\n",
    "\n",
    "aaa.select('all','topic_prin').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
