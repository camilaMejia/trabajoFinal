{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by step (Resumen)\n",
    "\n",
    "- Lanzamos cluster en aws\n",
    "- Nos conectamos por consola y ssh al cluster desde el pc personal\n",
    "- Instalamos jupyterhub y clonamos el  [repo](https://github.com/camilaMejia/trabajoFinal) del proyecto con este notebook listo. (En el github hay un archivo que se llama launch.txt donde están todas las intrucciones que se lanzan por comando)\n",
    "- Instalamos y cargamos todas las librerias necesarias.\n",
    "- Nos traemos el .dat y el .csv desde S3 al almacenamiento local\n",
    "- Creamos el indice invertido usando metapy\n",
    "- Hacemos querying usando BM25\n",
    "- Se hace un LDA con todos las noticias (solo content + title)\n",
    "- Para cada noticia hacemos vemos cual es el topico dominante\n",
    "- Asignación de la polaridad de cada noticia utilizando textBlob.\n",
    "\n",
    "\n",
    "\n",
    "# Procedimiento en AWS\n",
    "\n",
    "Uno de los objetivos de este trabajo estaba asociado a la posibilidad de correr todo esto en maquinas virtuales y clusters de AWS. Para ello lo que hicimos fue la creación de un cluster a traves de EMR y luego hacer un ambiente particular para instalar metapy.\n",
    "\n",
    "Una dificultad es que estos clusters son efimeros por lo que no almacenan los notebooks, por eso lo que se hizo es que cuando lanzamos el cluster además de instalar metapy y el resto de librerias necesarias instalamos github para clonar todos los archivos necesarios menos el .dat y el csv con todas las noticias (dado que son muy pesados).\n",
    "\n",
    "Para traer al ambiente los datos que son muy grandes lo que hacemos es copiarlos de una carpeta de aws.\n",
    "\n",
    "Para la parte de LDA con todos los datos era necesario correr spark en el cluster (no local) por eso para esta tarea se usaron los notebooks nativos de EMR. En la versión que vemos en este notebook en realidad solo tenemos un demo co 1000 noticias para que corra suficientmeente rapido (además que el EMR consume muchos más recursos y los 50USD no son suficiente).\n",
    "\n",
    "# Indexación y recuperación: metapy\n",
    "\n",
    "Usando la libreria metapy lo que hacemos es simplemente configurar el archivo llamado minifig.toml en el cual especificamos en donde están los documentos que vamos a indexar (junto con su formato) para luego hacer la recuperación.\n",
    "\n",
    "Lo que se hace en la sección de indexación es la creación del indice invertido que luego va a ser utilizado, en la sección de recuperación simplemente se crea un objeto  el cual genera un ranking (en este caso es un BM25) cuando se le pasa un query.\n",
    "\n",
    "# LDA: spark\n",
    "\n",
    "Lo que hacemos en este procedimiento es usar las funciones de spark para leer la información de todas las noticias para primero hacer la representación vectorial de los documentos, luego calculamos el indice invertido para finalmente hacer la reducción de dimencionalidad con LDA.\n",
    "\n",
    "LDA lo que hace es que asigna a cada noticia un vector de k posiciones en donde la posicion i  expresa la probabilidad de que la noticia hable del topico i. Además de esto a cada noticia se le puede asignar su topico dominante.\n",
    "\n",
    "Adicional vemos que cada topico tiene unas tokens que lo definen, dado que estos topicos no son predeterminados es necesario que se haga un analisis de qué es cada cluster con base en esos tokens representativos (algo parecido a cuando en k-means se hace un analisis de los centroides).\n",
    "\n",
    "Algo muy importante es que en este demo es necesario crear un contexto de spark (que además es local), por el contrario en EMR el contexto ya es predeterminado.\n",
    "\n",
    "# Sentiment Analysis\n",
    "\n",
    "Para esta sección la idea inicial era entrenar un algoritmo sencillo, pero dado que no hay un sentimiento asociado (label), optamos por la opción de utilizar una libreria que nos da una nivel de sentimiento en cada noticia según su polaridad (negativo o positivo).\n",
    "\n",
    "La idea es usar textBlob y con base en la distribución de la polaridad vamos a asignar un sentimiento a cada noticia: Negativo, neutral o positivo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalar librerias y complementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/74/e50234bc82c553fecdbd566d8650801e3fe2d6d8c8d940638e3d8a7c5522/pandas-0.24.2-cp36-cp36m-manylinux1_x86_64.whl (10.1MB)\n",
      "\u001b[K     |████████████████████████████████| 10.1MB 24.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2011k (from pandas)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/73/fe30c2daaaa0713420d0382b16fbb761409f532c56bdcc514bf7b6262bb6/pytz-2019.1-py2.py3-none-any.whl (510kB)\n",
      "\u001b[K     |████████████████████████████████| 512kB 24.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.12.0 (from pandas)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3MB 26.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.0 in /home/hadoop/env/lib/python3.6/dist-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/hadoop/env/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "Installing collected packages: pytz, numpy, pandas\n",
      "Successfully installed numpy-1.16.4 pandas-0.24.2 pytz-2019.1\n",
      "Collecting pyspark\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/98/244399c0daa7894cdf387e7007d5e8b3710a79b67f3fd991c0b0b644822d/pyspark-2.4.3.tar.gz (215.6MB)\n",
      "\u001b[K     |████████████████████████████████| 215.6MB 203kB/s  eta 0:00:01   |▊                               | 5.0MB 22.4MB/s eta 0:00:10     |██▌                             | 16.7MB 22.4MB/s eta 0:00:09     |█████████▏                      | 61.7MB 18.1MB/s eta 0:00:09     |██████████                      | 67.0MB 18.1MB/s eta 0:00:09     |██████████▌                     | 70.8MB 18.1MB/s eta 0:00:09     |███████████▊                    | 78.7MB 46.4MB/s eta 0:00:03     |████████████▌                   | 84.1MB 46.4MB/s eta 0:00:03     |████████████▉                   | 86.6MB 46.4MB/s eta 0:00:03     |█████████████                   | 88.0MB 46.4MB/s eta 0:00:03\n",
      "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 35.9MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/hadoop/.cache/pip/wheels/8d/20/f0/b30e2024226dc112e256930dd2cd4f06d00ab053c86278dcf3\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.3\n",
      "Collecting metapy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/a4/92dae084446597d6bbf355e7eaff3e83dcb51e33d434f43ecdea4c0c4b0a/metapy-0.2.13-cp36-cp36m-manylinux1_x86_64.whl (14.3MB)\n",
      "\u001b[K     |████████████████████████████████| 14.3MB 26.8MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: metapy\n",
      "Successfully installed metapy-0.2.13\n",
      "Collecting boto3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/bd/eaa273e81e40d2604bd5473a80e040c6943a2f79a18896a65e7296f9fea2/boto3-1.9.173-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 23.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting botocore<1.13.0,>=1.12.173 (from boto3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/bb/226c21c8ff37c620412280e71dd7b0135c50d380ac212e3cd0c34d4bc6ef/botocore-1.12.173-py2.py3-none-any.whl (5.6MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6MB 26.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.3.0,>=0.2.0 (from boto3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl (70kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 23.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docutils>=0.10 (from botocore<1.13.0,>=1.12.173->boto3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
      "\u001b[K     |████████████████████████████████| 552kB 39.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/hadoop/env/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.173->boto3) (2.8.0)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version >= \"3.4\" in /home/hadoop/env/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.173->boto3) (1.25.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/hadoop/env/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.173->boto3) (1.12.0)\n",
      "Installing collected packages: jmespath, docutils, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.9.173 botocore-1.12.173 docutils-0.14 jmespath-0.9.4 s3transfer-0.2.1\n",
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/5d/825889810b85c303c8559a3fd74d451d80cf3585a851f2103e69576bf583/nltk-3.4.3.zip (1.4MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 33.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/hadoop/env/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/hadoop/.cache/pip/wheels/54/40/b7/c56ad418e6cd4d9e1e594b5e138d1ca6eec11a6ee3d464e5bb\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.4.3\n",
      "Requirement already satisfied: numpy in /home/hadoop/env/lib64/python3.6/site-packages (1.16.4)\n",
      "Collecting matplotlib\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/83/d989ee20c78117c737ab40e0318ea221f1aed4e3f5a40b4f93541b369b93/matplotlib-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (13.1MB)\n",
      "\u001b[K     |████████████████████████████████| 13.1MB 29.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /home/hadoop/env/lib/python3.6/dist-packages (from matplotlib) (2.8.0)\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/d9/3ec19e966301a6e25769976999bd7bbe552016f0d32b577dc9d63d2e0c49/pyparsing-2.4.0-py2.py3-none-any.whl (62kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 25.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/a1/5742b56282449b1c0968197f63eae486eca2c35dcd334bab75ad524e0de1/kiwisolver-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (90kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 25.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11 in /home/hadoop/env/lib64/python3.6/site-packages (from matplotlib) (1.16.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/hadoop/env/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /home/hadoop/env/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (41.0.1)\n",
      "Installing collected packages: pyparsing, cycler, kiwisolver, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.1.0 matplotlib-3.1.0 pyparsing-2.4.0\n",
      "Collecting TextBlob\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/f0/1d9bfcc8ee6b83472ec571406bd0dd51c0e6330ff1a51b2d29861d389e85/textblob-0.15.3-py2.py3-none-any.whl (636kB)\n",
      "\u001b[K     |████████████████████████████████| 645kB 26.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /home/hadoop/env/lib/python3.6/site-packages (from TextBlob) (3.4.3)\n",
      "Requirement already satisfied: six in /home/hadoop/env/lib/python3.6/dist-packages (from nltk>=3.1->TextBlob) (1.12.0)\n",
      "Installing collected packages: TextBlob\n",
      "Successfully installed TextBlob-0.15.3\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas\n",
    "! pip install pyspark\n",
    "! pip install metapy\n",
    "! pip install boto3\n",
    "! pip install nltk\n",
    "! pip install numpy\n",
    "! pip install matplotlib\n",
    "! pip install TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "import metapy\n",
    "import requests, zipfile, io, os, boto3\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    " \n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA, BisectingKMeans\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import re\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "sc = SparkContext('local', \"app-topic-detection\") \n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar datos necesarios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 =  boto3.client('s3', region_name='us-east-1')\n",
    "with open('data/news/news.dat', 'wb') as f:\n",
    "    s3.download_fileobj('finaltext','news.dat', f)\n",
    "\n",
    "\n",
    "\n",
    "obj = s3.get_object(Bucket='finaltext', Key=u'news.csv')\n",
    "df = pd.read_csv(obj['Body'])\n",
    "\n",
    "df['all']=df.title + df.content\n",
    "\n",
    "df2=df[['all']]\n",
    "df2.to_csv('aux.csv')\n",
    "df2.head(1000).to_csv('mini.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted index using metapy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de documentos: 142570\n",
      "Cantidad de palabras únicas: 298498\n",
      "Promedio de longitud de los documentos: 378.13653564453125\n"
     ]
    }
   ],
   "source": [
    "#!rm -rf news-idx\n",
    "idx = metapy.index.make_inverted_index('miniconfig.toml')\n",
    "\n",
    "print(f'Total de documentos: {idx.num_docs()}')\n",
    "print(f'Cantidad de palabras únicas: {idx.unique_terms()}')\n",
    "print(f'Promedio de longitud de los documentos: {idx.avg_doc_length()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR: Querys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(23373, 11.945362091064453), (55967, 10.423839569091797), (123632, 10.146553993225098), (119243, 9.89953327178955), (25871, 9.880447387695312)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23373</th>\n",
       "      <td>Ann Coulter: I Might Have Been Killed at the B...</td>\n",
       "      <td>Ann Coulter, in a major interview with Vanity ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55967</th>\n",
       "      <td>A Trump Supporter Dwells in Beijing</td>\n",
       "      <td>BEIJING —  Ardent Chinese supporters of Donald...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123632</th>\n",
       "      <td>For Chinese officials, Trump perhaps better th...</td>\n",
       "      <td>In 2010, then Secretary of State Hillary Clin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119243</th>\n",
       "      <td>Massachusetts college apologizes for racist tw...</td>\n",
       "      <td>Salem State University President Patricia Mese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25871</th>\n",
       "      <td>Donald Trump’s Hypocrisies - Breitbart</td>\n",
       "      <td>Part of Donald Trump’s appeal as a candidate i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "23373   Ann Coulter: I Might Have Been Killed at the B...   \n",
       "55967                 A Trump Supporter Dwells in Beijing   \n",
       "123632  For Chinese officials, Trump perhaps better th...   \n",
       "119243  Massachusetts college apologizes for racist tw...   \n",
       "25871              Donald Trump’s Hypocrisies - Breitbart   \n",
       "\n",
       "                                                  content  \n",
       "23373   Ann Coulter, in a major interview with Vanity ...  \n",
       "55967   BEIJING —  Ardent Chinese supporters of Donald...  \n",
       "123632   In 2010, then Secretary of State Hillary Clin...  \n",
       "119243  Salem State University President Patricia Mese...  \n",
       "25871   Part of Donald Trump’s appeal as a candidate i...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker = metapy.index.OkapiBM25(k1=1.2, b=0.75)\n",
    "query = metapy.index.Document()\n",
    "query.content('Trump hates china') # query from AP news\n",
    "top_docs = ranker.score(idx, query, num_results=5)\n",
    "\n",
    "print(top_docs)\n",
    "index=[tup[0] for tup in top_docs]\n",
    "df.loc[index,['title','content']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA on spark\n",
    "\n",
    "### Pre process data\n",
    "\n",
    "Here we load data to spark and make some preprocessing over the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "|_c0|                 all|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|House Republicans...|[hous, republican...|\n",
      "|  1|Rift Between Offi...|[rift, offic, res...|\n",
      "|  2|Tyrus Wong, ‘Bamb...|[tyru, wong, bamb...|\n",
      "|  3|Among Deaths in 2...|[among, death, 20...|\n",
      "|  4|Kim Jong-un Says ...|[kim, jongun, say...|\n",
      "|  5|Sick With a Cold,...|[sick, cold, quee...|\n",
      "|  6|Taiwan’s Presiden...|[taiwan, presid, ...|\n",
      "|  7|After ‘The Bigges...|[biggest, loser, ...|\n",
      "|  8|First, a Mixtape....|[first, mixtap, r...|\n",
      "|  9|Calling on Angels...|[call, angel, end...|\n",
      "| 10|Weak Federal Powe...|[weak, feder, pow...|\n",
      "| 11|Can Carbon Captur...|[carbon, captur, ...|\n",
      "| 12|Mar-a-Lago, the F...|[maralago, futur,...|\n",
      "| 13|How to form healt...|[form, healthi, h...|\n",
      "| 14|Turning Your Vaca...|[turn, vacat, pho...|\n",
      "| 15|As Second Avenue ...|[second, avenu, s...|\n",
      "| 16|Dylann Roof Himse...|[dylann, roof, re...|\n",
      "| 17|Modi’s Cash Ban B...|[modi, cash, ban,...|\n",
      "| 18|Suicide Bombing i...|[suicid, bomb, ba...|\n",
      "| 19|Fecal Pollution T...|[fecal, pollut, t...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#rawdata=spark.read.csv('aux.csv', inferSchema=True, header=True)\n",
    "\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "rawdata = spark.read.load(\"mini.csv\", format=\"csv\", header=True)\n",
    "\n",
    "rawdata[\"all\"].cast(StringType())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cleanup_text(record):\n",
    "    text  = record[1]\n",
    "    uid   = record[0]\n",
    "    \n",
    "    # Default list of Stopwords\n",
    "    sw = list(stop_words_nltk)\n",
    "    \n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [ re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens ] #Just letters and numbers\n",
    "    tokens = [ w.lower() for w in tokens] #lowercase\n",
    "    tokens = [ w for w in tokens if (len(w)>1) ] #Not single letter words\n",
    "    tokens = [ w for w in tokens if w not in sw ] #Remove stopwords\n",
    "    stemmer = PorterStemmer ()\n",
    "    tokens =[stemmer.stem(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "udf_cleantext = udf(cleanup_text , ArrayType(StringType()))\n",
    "clean_text = rawdata.withColumn(\"words\", udf_cleantext(struct([rawdata[x] for x in rawdata.columns])))\n",
    "clean_text.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedings + LDA\n",
    "\n",
    "here we create the features of each line and then make the LDA itself with k topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequency Vectorization  - Option 2 (CountVectorizer)    : \n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\", vocabSize = 1000)\n",
    "cvmodel = cv.fit(clean_text)\n",
    "featurizedData = cvmodel.transform(clean_text)\n",
    "\n",
    "vocab = cvmodel.vocabulary\n",
    "vocab_broadcast = sc.broadcast(vocab)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "# Generate 25 Data-Driven Topics:\n",
    "lda = LDA(k=5, seed=123, optimizer=\"em\", featuresCol=\"features\")\n",
    "\n",
    "ldamodel = lda.fit(rescaledData)\n",
    "\n",
    "#model.isDistributed()\n",
    "#model.vocabSize()\n",
    "\n",
    "ldatopics = ldamodel.describeTopics()\n",
    "#ldatopics.show(25)\n",
    "\n",
    "def map_termID_to_Word(termIndices):\n",
    "    words = []\n",
    "    for termID in termIndices:\n",
    "        words.append(vocab_broadcast.value[termID])\n",
    "    \n",
    "    return words\n",
    "\n",
    "udf_map_termID_to_Word = udf(map_termID_to_Word , ArrayType(StringType()))\n",
    "ldatopics_mapped = ldatopics.withColumn(\"topic_desc\", udf_map_termID_to_Word(ldatopics.termIndices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------+\n",
      "|topic|topic_desc                                                                    |\n",
      "+-----+------------------------------------------------------------------------------+\n",
      "|0    |[polic, attack, women, ms, march, kill, museum, drug, black, dr]              |\n",
      "|1    |[trump, republican, russia, russian, health, mr, intellig, obama, insur, care]|\n",
      "|2    |[china, unit, trade, refuge, trump, state, european, iran, mexico, countri]   |\n",
      "|3    |[judg, court, senat, feder, compani, justic, school, gorsuch, rule, law]      |\n",
      "|4    |[neanderth, book, game, food, play, app, data, player, season, chines]        |\n",
      "+-----+------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ldatopics_mapped.select(ldatopics_mapped.topic, ldatopics_mapped.topic_desc).show(50,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add detected topic to each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                 all|               words|            features|   topicDistribution|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|House Republicans...|[hous, republican...|(1000,[0,1,2,3,4,...|[0.04068090294573...|\n",
      "|Rift Between Offi...|[rift, offic, res...|(1000,[0,1,3,4,5,...|[0.75246243207529...|\n",
      "|Tyrus Wong, ‘Bamb...|[tyru, wong, bamb...|(1000,[0,1,3,4,5,...|[0.41298940795203...|\n",
      "|Among Deaths in 2...|[among, death, 20...|(1000,[0,1,3,4,5,...|[0.26343186446165...|\n",
      "|Kim Jong-un Says ...|[kim, jongun, say...|(1000,[0,1,2,3,5,...|[0.05080163588748...|\n",
      "|Sick With a Cold,...|[sick, cold, quee...|(1000,[1,3,6,9,13...|[0.32384504591293...|\n",
      "|Taiwan’s Presiden...|[taiwan, presid, ...|(1000,[0,1,2,3,4,...|[0.08547109041080...|\n",
      "|After ‘The Bigges...|[biggest, loser, ...|(1000,[0,1,3,4,5,...|[0.26709619308269...|\n",
      "|First, a Mixtape....|[first, mixtap, r...|(1000,[0,1,3,4,6,...|[0.37183295839206...|\n",
      "|Calling on Angels...|[call, angel, end...|(1000,[0,1,3,4,6,...|[0.39040127243799...|\n",
      "|Weak Federal Powe...|[weak, feder, pow...|(1000,[0,1,2,3,4,...|[0.07898245402297...|\n",
      "|Can Carbon Captur...|[carbon, captur, ...|(1000,[0,1,2,3,4,...|[0.04296681289651...|\n",
      "|Mar-a-Lago, the F...|[maralago, futur,...|(1000,[0,1,2,3,4,...|[0.14957863659083...|\n",
      "|How to form healt...|[form, healthi, h...|(1000,[3,4,6,7,9,...|[0.14672466280298...|\n",
      "|Turning Your Vaca...|[turn, vacat, pho...|(1000,[1,3,4,5,6,...|[0.05592426976958...|\n",
      "|As Second Avenue ...|[second, avenu, s...|(1000,[0,1,3,4,5,...|[0.39197352949864...|\n",
      "|Dylann Roof Himse...|[dylann, roof, re...|(1000,[0,1,3,4,5,...|[0.36863960494812...|\n",
      "|Modi’s Cash Ban B...|[modi, cash, ban,...|(1000,[0,1,2,3,4,...|[0.13276980742614...|\n",
      "|Suicide Bombing i...|[suicid, bomb, ba...|(1000,[0,1,3,6,7,...|[0.34901162108741...|\n",
      "|Fecal Pollution T...|[fecal, pollut, t...|(1000,[0,1,3,7,10...|[0.17957766484493...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ldaResults = ldamodel.transform(rescaledData)\n",
    "\n",
    "ldaResults.select('all','words','features','topicDistribution').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add principal topic to each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                 all|topic_prin|\n",
      "+--------------------+----------+\n",
      "|House Republicans...|         1|\n",
      "|Rift Between Offi...|         0|\n",
      "|Tyrus Wong, ‘Bamb...|         0|\n",
      "|Among Deaths in 2...|         4|\n",
      "|Kim Jong-un Says ...|         2|\n",
      "|Sick With a Cold,...|         0|\n",
      "|Taiwan’s Presiden...|         2|\n",
      "|After ‘The Bigges...|         4|\n",
      "|First, a Mixtape....|         4|\n",
      "|Calling on Angels...|         0|\n",
      "|Weak Federal Powe...|         3|\n",
      "|Can Carbon Captur...|         3|\n",
      "|Mar-a-Lago, the F...|         4|\n",
      "|How to form healt...|         4|\n",
      "|Turning Your Vaca...|         4|\n",
      "|As Second Avenue ...|         0|\n",
      "|Dylann Roof Himse...|         3|\n",
      "|Modi’s Cash Ban B...|         2|\n",
      "|Suicide Bombing i...|         2|\n",
      "|Fecal Pollution T...|         4|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "def foo(topicDistribution):\n",
    "    dom = topicDistribution[0]\n",
    "    index_dom = 0\n",
    "    for index in range(len(topicDistribution)):\n",
    "        if (topicDistribution[index]>dom):\n",
    "            dom=topicDistribution[index]\n",
    "            index_dom=index\n",
    "    return index_dom\n",
    "\n",
    "udf_seltop = udf(foo , IntegerType())\n",
    "aaa = ldaResults.withColumn(\"topic_prin\", udf_seltop(ldaResults.topicDistribution))\n",
    "\n",
    "\n",
    "aaa.select('all','topic_prin').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 s, sys: 13.4 ms, total: 14 s\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def p_pol(x):\n",
    "    if x > p66:\n",
    "        return \"positivo\"\n",
    "    elif x < p33:\n",
    "        return \"negativo\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "def sentiment_func(news):\n",
    "    try:\n",
    "        return TextBlob(news).sentiment\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "df_pandas=aaa.select('all','topic_prin').toPandas()\n",
    "\n",
    "df=df2.head(df_pandas.shape[0]).copy()\n",
    "    \n",
    "%time df['sentimientos'] = df['all'].apply(sentiment_func)\n",
    "\n",
    "df['polaridad'] = df['sentimientos'].apply(lambda x: x[0])\n",
    "df['Subjetividad'] = df['sentimientos'].apply(lambda x: x[1])\n",
    "\n",
    "\n",
    "p33=np.percentile(df['polaridad'],33)\n",
    "p66=np.percentile(df['polaridad'],66)\n",
    "\n",
    "df['feeling']=df['polaridad'].apply(p_pol)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df=df[['all','polaridad','Subjetividad','feeling']].merge(df_pandas)\n",
    "\n",
    "df.to_csv('Sentiments_news.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
