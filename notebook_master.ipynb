{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qué hemos hecho?\n",
    "\n",
    "- Lanzamos cluster en aws\n",
    "- Nos conectamos por consola y ssh al cluster desde el pc personal\n",
    "- Instalamos jupyterhub y clonamos el  [repo](https://github.com/camilaMejia/trabajoFinal) del proyecto con este notebook listo. (En el github hay un archivo que se llama launch.txt donde están todas las intrucciones que se lanzan por comando)\n",
    "- Instalamos y cargamos todas las librerias necesarias.\n",
    "- Nos traemos el .dat y el .csv desde S3 al almacenamiento local\n",
    "- Creamos el indice invertido usando metapy\n",
    "- Hacemos querying usando BM25\n",
    "- Se hace un LDA con todos las noticias (solo content + title)\n",
    "- Para cada noticia hacemos vemos cual es el topico dominante\n",
    "\n",
    "# Con respecto a la entrega\n",
    "\n",
    "- Almacenamiento y Cluster de procesamiento en SparkML/Meta/NLTK en Amazon AWS : Check (Peso 40%)\n",
    "- Indexación, búsqueda y recuperación con META : Check (Peso 40%)\n",
    "- Modelado de tópicos: Check (Peso 10%)\n",
    "- Análisis de sentimientos: Pendiente (Peso 10%)\n",
    "\n",
    "En general estamos al 90% de ejecución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalar librerias y complementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached https://files.pythonhosted.org/packages/19/74/e50234bc82c553fecdbd566d8650801e3fe2d6d8c8d940638e3d8a7c5522/pandas-0.24.2-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting numpy>=1.12.0 (from pandas)\n",
      "  Using cached https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /home/hadoop/env/lib/python3.6/dist-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in /home/hadoop/env/lib/python3.6/dist-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/hadoop/env/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "Installing collected packages: numpy, pandas\n",
      "Successfully installed numpy-1.16.4 pandas-0.24.2\n",
      "Requirement already satisfied: pyspark in /home/hadoop/env/lib/python3.6/dist-packages (2.4.3)\n",
      "Requirement already satisfied: py4j==0.10.7 in /home/hadoop/env/lib/python3.6/dist-packages (from pyspark) (0.10.7)\n",
      "Collecting metapy\n",
      "  Using cached https://files.pythonhosted.org/packages/81/a4/92dae084446597d6bbf355e7eaff3e83dcb51e33d434f43ecdea4c0c4b0a/metapy-0.2.13-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Installing collected packages: metapy\n",
      "Successfully installed metapy-0.2.13\n",
      "Collecting boto3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/1f/b272ead5ccc5370717f3c65ebd5092feab90e748db041bd96c565e7d1a72/boto3-1.9.169-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 31.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting botocore<1.13.0,>=1.12.169 (from boto3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/ac/a43d37f371f5854514128d7c54887176b8df3bc9925a25e5096298033f93/botocore-1.12.169-py2.py3-none-any.whl (5.5MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6MB 34.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.3.0,>=0.2.0 (from boto3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl (70kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 22.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docutils>=0.10 (from botocore<1.13.0,>=1.12.169->boto3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
      "\u001b[K     |████████████████████████████████| 552kB 25.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.26,>=1.20; python_version >= \"3.4\" in /home/hadoop/env/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.169->boto3) (1.25.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/hadoop/env/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.169->boto3) (2.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/hadoop/env/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.169->boto3) (1.12.0)\n",
      "Installing collected packages: jmespath, docutils, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.9.169 botocore-1.12.169 docutils-0.14 jmespath-0.9.4 s3transfer-0.2.1\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas\n",
    "! pip install pyspark\n",
    "! pip install metapy\n",
    "! pip install boto3\n",
    "!pip install nltk\n",
    "!pip install numpy\n",
    "!pip install re\n",
    "!pip install codecs\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "import metapy\n",
    "import requests, zipfile, io, os, boto3\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA, BisectingKMeans\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import re\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local', \"app-topic-detection\") \n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar datos necesarios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 =  boto3.client('s3', region_name='us-east-1')\n",
    "with open('data/news/news.dat', 'wb') as f:\n",
    "    s3.download_fileobj('finaltext','news.dat', f)\n",
    "\n",
    "obj = s3.get_object(Bucket='finaltext', Key=u'news.csv')\n",
    "df = pd.read_csv(obj['Body'])\n",
    "\n",
    "df['all']=df.title + df.content\n",
    "df[['all']].head(20000).to_csv('aux.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted index using metapy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf news-idx\n",
    "idx = metapy.index.make_inverted_index('miniconfig.toml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR: Querys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23373</th>\n",
       "      <td>Ann Coulter: I Might Have Been Killed at the B...</td>\n",
       "      <td>Ann Coulter, in a major interview with Vanity ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55967</th>\n",
       "      <td>A Trump Supporter Dwells in Beijing</td>\n",
       "      <td>BEIJING —  Ardent Chinese supporters of Donald...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123632</th>\n",
       "      <td>For Chinese officials, Trump perhaps better th...</td>\n",
       "      <td>In 2010, then Secretary of State Hillary Clin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119243</th>\n",
       "      <td>Massachusetts college apologizes for racist tw...</td>\n",
       "      <td>Salem State University President Patricia Mese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25871</th>\n",
       "      <td>Donald Trump’s Hypocrisies - Breitbart</td>\n",
       "      <td>Part of Donald Trump’s appeal as a candidate i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "23373   Ann Coulter: I Might Have Been Killed at the B...   \n",
       "55967                 A Trump Supporter Dwells in Beijing   \n",
       "123632  For Chinese officials, Trump perhaps better th...   \n",
       "119243  Massachusetts college apologizes for racist tw...   \n",
       "25871              Donald Trump’s Hypocrisies - Breitbart   \n",
       "\n",
       "                                                  content  \n",
       "23373   Ann Coulter, in a major interview with Vanity ...  \n",
       "55967   BEIJING —  Ardent Chinese supporters of Donald...  \n",
       "123632   In 2010, then Secretary of State Hillary Clin...  \n",
       "119243  Salem State University President Patricia Mese...  \n",
       "25871   Part of Donald Trump’s appeal as a candidate i...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker = metapy.index.OkapiBM25()\n",
    "query = metapy.index.Document()\n",
    "query.content('Trump hates china') # query from AP news\n",
    "top_docs = ranker.score(idx, query, num_results=5)\n",
    "\n",
    "index=[tup[0] for tup in top_docs]\n",
    "df.loc[index,['title','content']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA on spark\n",
    "\n",
    "### Pre process data\n",
    "\n",
    "Here we load data to spark and make some preprocessing over the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv('aux.csv', inferSchema=True, header=True)\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# stopwords en nltk\n",
    "\n",
    "\n",
    "\n",
    "rawdata = spark.read.load(\"aux.csv\", format=\"csv\", header=True)\n",
    "\n",
    "\n",
    "def cleanup_text(record):\n",
    "    text  = record[1]\n",
    "    uid   = record[0]\n",
    "    words = text.split()\n",
    "    \n",
    "    # Default list of Stopwords\n",
    "    stopwords_core = ['a', u'about', u'above', u'after', u'again', u'against', u'all', u'am', u'an', u'and', u'any', u'are', u'arent', u'as', u'at', \n",
    "    u'be', u'because', u'been', u'before', u'being', u'below', u'between', u'both', u'but', u'by', \n",
    "    u'can', 'cant', 'come', u'could', 'couldnt', \n",
    "    u'd', u'did', u'didn', u'do', u'does', u'doesnt', u'doing', u'dont', u'down', u'during', \n",
    "    u'each', \n",
    "    u'few', 'finally', u'for', u'from', u'further', \n",
    "    u'had', u'hadnt', u'has', u'hasnt', u'have', u'havent', u'having', u'he', u'her', u'here', u'hers', u'herself', u'him', u'himself', u'his', u'how', \n",
    "    u'i', u'if', u'in', u'into', u'is', u'isnt', u'it', u'its', u'itself', \n",
    "    u'just', \n",
    "    u'll', \n",
    "    u'm', u'me', u'might', u'more', u'most', u'must', u'my', u'myself', \n",
    "    u'no', u'nor', u'not', u'now', \n",
    "    u'o', u'of', u'off', u'on', u'once', u'only', u'or', u'other', u'our', u'ours', u'ourselves', u'out', u'over', u'own', \n",
    "    u'r', u're', \n",
    "    u's', 'said', u'same', u'she', u'should', u'shouldnt', u'so', u'some', u'such', \n",
    "    u't', u'than', u'that', 'thats', u'the', u'their', u'theirs', u'them', u'themselves', u'then', u'there', u'these', u'they', u'this', u'those', u'through', u'to', u'too', \n",
    "    u'under', u'until', u'up', \n",
    "    u'very', \n",
    "    u'was', u'wasnt', u'we', u'were', u'werent', u'what', u'when', u'where', u'which', u'while', u'who', u'whom', u'why', u'will', u'with', u'wont', u'would', \n",
    "    u'y', u'you', u'your', u'yours', u'yourself', u'yourselves']\n",
    "    \n",
    "    # Custom List of Stopwords - Add your own here\n",
    "    stopwords_custom = ['']\n",
    "    stopwords = stopwords_core + stopwords_custom\n",
    "    stopwords = [word.lower() for word in stopwords]    \n",
    "    \n",
    "    text_out = [re.sub('[^a-zA-Z0-9]','',word) for word in words]                                       # Remove special characters\n",
    "    text_out = [word.lower() for word in text_out if len(word)>2 and word.lower() not in stopwords]     # Remove stopwords and words under X length\n",
    "    return text_out\n",
    "\n",
    "udf_cleantext = udf(cleanup_text , ArrayType(StringType()))\n",
    "clean_text = rawdata.withColumn(\"words\", udf_cleantext(struct([rawdata[x] for x in rawdata.columns])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedings + LDA\n",
    "\n",
    "here we create the features of each line and then make the LDA itself with k topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Term Frequency Vectorization  - Option 2 (CountVectorizer)    : \n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\", vocabSize = 1000)\n",
    "cvmodel = cv.fit(clean_text)\n",
    "featurizedData = cvmodel.transform(clean_text)\n",
    "\n",
    "vocab = cvmodel.vocabulary\n",
    "vocab_broadcast = sc.broadcast(vocab)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "# Generate 25 Data-Driven Topics:\n",
    "lda = LDA(k=5, seed=123, optimizer=\"em\", featuresCol=\"features\")\n",
    "\n",
    "ldamodel = lda.fit(rescaledData)\n",
    "\n",
    "#model.isDistributed()\n",
    "#model.vocabSize()\n",
    "\n",
    "ldatopics = ldamodel.describeTopics()\n",
    "#ldatopics.show(25)\n",
    "\n",
    "def map_termID_to_Word(termIndices):\n",
    "    words = []\n",
    "    for termID in termIndices:\n",
    "        words.append(vocab_broadcast.value[termID])\n",
    "    \n",
    "    return words\n",
    "\n",
    "udf_map_termID_to_Word = udf(map_termID_to_Word , ArrayType(StringType()))\n",
    "ldatopics_mapped = ldatopics.withColumn(\"topic_desc\", udf_map_termID_to_Word(ldatopics.termIndices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldatopics_mapped.select(ldatopics_mapped.topic, ldatopics_mapped.topic_desc).show(50,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add detected topic to each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|               title|             content|               words|            features|   topicDistribution|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|House Republicans...|WASHINGTON  —   C...|[house, republica...|(1000,[0,1,2,20,2...|[0.10604047711236...|\n",
      "|Rift Between Offi...|After the bullet ...|[rift, officers, ...|(1000,[0,1,2,31,3...|[0.09065335868619...|\n",
      "|Tyrus Wong, ‘Bamb...|When Walt Disney’...|[tyrus, wong, bam...|(1000,[0,1,2,11,7...|[0.08040092480352...|\n",
      "|Among Deaths in 2...|Death may be the ...|[among, deaths, 2...|(1000,[0,1,2,208,...|[0.06378516439272...|\n",
      "|Kim Jong-un Says ...|SEOUL, South Kore...|[kim, jongun, say...|(1000,[0,1,2,7,21...|[0.08504651510476...|\n",
      "|Sick With a Cold,...|LONDON  —   Queen...|[sick, cold, quee...|(1000,[0,1,2,81,6...|[0.22573492592097...|\n",
      "|Taiwan’s Presiden...|BEIJING  —   Pres...|[taiwans, preside...|(1000,[0,1,2,18,2...|[0.09411067608536...|\n",
      "|After ‘The Bigges...|Danny Cahill stoo...|[biggest, loser, ...|(1000,[0,1,2,711]...|[0.09061518686006...|\n",
      "|First, a Mixtape....|Just how   is Hil...|[first, mixtape, ...|(1000,[0,1,2,16],...|[0.10172383315341...|\n",
      "|Calling on Angels...|Angels are everyw...|[calling, angels,...|(1000,[0,1,2,290]...|[0.14699170799418...|\n",
      "|Weak Federal Powe...|With Donald J. Tr...|[weak, federal, p...|(1000,[0,1,2,5,28...|[0.09527754590933...|\n",
      "|Can Carbon Captur...|THOMPSONS, Tex.  ...|[carbon, capture,...|(1000,[0,1,2,3],[...|[0.09952481811883...|\n",
      "|Mar-a-Lago, the F...|WEST PALM BEACH, ...|[maralago, future...|(1000,[0,1,2,3,20...|[0.12853562238471...|\n",
      "|How to form healt...|This article is p...|[form, healthy, h...|(1000,[0,1,2],[0....|[0.0,0.0,0.0,0.0,...|\n",
      "|Turning Your Vaca...|It’s the season f...|[turning, vacatio...|(1000,[0,1,2,91,8...|[0.08828032540915...|\n",
      "|As Second Avenue ...|Finally. The Seco...|[second, avenue, ...|(1000,[0,1,2,399,...|[0.07237180433266...|\n",
      "|Dylann Roof Himse...|  pages into the ...|[dylann, roof, re...|(1000,[0,1,2,146,...|[0.12697825284723...|\n",
      "|Modi’s Cash Ban B...|MUMBAI, India  — ...|[modis, cash, ban...|(1000,[0,1,2,38,7...|[0.14862545985813...|\n",
      "|Suicide Bombing i...|BAGHDAD  —   A su...|[suicide, bombing...|(1000,[0,1,2,144,...|[0.07977824569680...|\n",
      "|Fecal Pollution T...|SYDNEY, Australia...|[fecal, pollution...|(1000,[0,1,2,590]...|[0.09678555412829...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ldaResults = ldamodel.transform(rescaledData)\n",
    "\n",
    "ldaResults.select('all','words','features','topicDistribution').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
